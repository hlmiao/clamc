{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: xgboost in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.4.2)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install xgboost -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "directory = '/home/ec2-user/SageMaker/xgboost-new'\n",
    "if directory not in sys.path:\n",
    "    sys.path.append(directory)\n",
    "\n",
    "job_name = 'grid_search'\n",
    "image_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451049120500.dkr.ecr.cn-northwest-1.amazonaws.com.cn/sagemaker-xgboost:1.2-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "aws_default_region = session.boto_session.region_name\n",
    "aws_account_id = session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "ecs = boto3.client('ecs')\n",
    "\n",
    "from sagemaker import image_uris\n",
    "print(image_uris.retrieve(framework='xgboost',region='cn-northwest-1',version='1.2-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!aws ecr get-login-password --region {aws_default_region} | docker login --username AWS --password-stdin 451049120500.dkr.ecr.cn-northwest-1.amazonaws.com.cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/ec2-user/SageMaker/xgboost-new/grid_search/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {directory}/{job_name}/Dockerfile\n",
    "FROM 451049120500.dkr.ecr.cn-northwest-1.amazonaws.com.cn/sagemaker-xgboost:1.2-1\n",
    "\n",
    "RUN pip --no-cache-dir install -i https://pypi.tuna.tsinghua.edu.cn/simple \\\n",
    "        boto3 \\\n",
    "        pandas \\\n",
    "        sklearn\n",
    "\n",
    "RUN curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" && unzip awscliv2.zip && ./aws/install && rm awscliv2.zip && rm -rf aws/\n",
    "\n",
    "ENV ENVIRONMENT=\"/home/environment\"\n",
    "RUN mkdir -p $ENVIRONMENT/input\n",
    "RUN mkdir -p $ENVIRONMENT/output\n",
    "COPY job.py $ENVIRONMENT/\n",
    "CMD aws s3 cp s3://sagemaker-cn-northwest-1-384680359421/grid_search/$TIMESTAMP/trial_$TRIAL/input/ $ENVIRONMENT/input/ --recursive && python $ENVIRONMENT/job.py && aws s3 cp $ENVIRONMENT/output/ s3://sagemaker-cn-northwest-1-384680359421/grid_search/$TIMESTAMP/trial_$TRIAL/output/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/ec2-user/SageMaker/xgboost-new/grid_search/job.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {directory}/{job_name}/job.py\n",
    "import json\n",
    "import pandas as pd\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "__start__ = timer()\n",
    "\n",
    "\n",
    "with open('/home/environment/input/param.json', 'r') as fin:\n",
    "    param = json.load(fin)\n",
    "print(param)\n",
    "model = XGBClassifier(random_state=0, use_label_encoder=False)\n",
    "m = model.set_params(**param)\n",
    "train_X = pd.read_csv('/home/environment/input/train_X.csv', index_col=0)\n",
    "train_Y = pd.read_csv('/home/environment/input/train_Y.csv', index_col=0)\n",
    "score = cross_val_score(m, train_X, train_Y, scoring='roc_auc', cv=5, n_jobs=-1).mean()\n",
    "\n",
    "# 输出\n",
    "score = {'score': score}\n",
    "with open('/home/environment/output/output.json', 'w') as fout:\n",
    "    json.dump(score, fout)\n",
    "\n",
    "\n",
    "run_time = timer() - __start__\n",
    "print(\"Run time %f seconds \" % run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'grid_search' already exists in the registry with id '384680359421'\n"
     ]
    }
   ],
   "source": [
    "!aws ecr create-repository --repository-name {job_name} --region {aws_default_region} > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/8 : FROM 451049120500.dkr.ecr.cn-northwest-1.amazonaws.com.cn/sagemaker-xgboost:1.2-1\n",
      " ---> 87cbbea7628d\n",
      "Step 2/8 : RUN pip --no-cache-dir install -i https://pypi.tuna.tsinghua.edu.cn/simple         boto3         pandas         sklearn\n",
      " ---> Using cache\n",
      " ---> 935f21d214c0\n",
      "Step 3/8 : RUN curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" && unzip awscliv2.zip && ./aws/install && rm awscliv2.zip && rm -rf aws/\n",
      " ---> Using cache\n",
      " ---> 6486645370a0\n",
      "Step 4/8 : ENV ENVIRONMENT=\"/home/environment\"\n",
      " ---> Using cache\n",
      " ---> 63077328f2a7\n",
      "Step 5/8 : RUN mkdir -p $ENVIRONMENT/input\n",
      " ---> Using cache\n",
      " ---> 7f85f4f6555d\n",
      "Step 6/8 : RUN mkdir -p $ENVIRONMENT/output\n",
      " ---> Using cache\n",
      " ---> 7e187e9dfec3\n",
      "Step 7/8 : COPY job.py $ENVIRONMENT/\n",
      " ---> Using cache\n",
      " ---> 770f7ae9d132\n",
      "Step 8/8 : CMD aws s3 cp s3://sagemaker-cn-northwest-1-384680359421/grid_search/$TIMESTAMP/trial_$TRIAL/input/ $ENVIRONMENT/input/ --recursive && python $ENVIRONMENT/job.py && aws s3 cp $ENVIRONMENT/output/ s3://sagemaker-cn-northwest-1-384680359421/grid_search/$TIMESTAMP/trial_$TRIAL/output/ --recursive\n",
      " ---> Using cache\n",
      " ---> 9e5b731ef09b\n",
      "Successfully built 9e5b731ef09b\n",
      "Successfully tagged grid_search:latest\n"
     ]
    }
   ],
   "source": [
    "!cd {directory} && docker build {job_name} -t {job_name}:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Pushing image\n",
      "The push refers to repository [384680359421.dkr.ecr.cn-northwest-1.amazonaws.com.cn/grid_search]\n",
      "\n",
      "\u001b[1B2e388952: Preparing \n",
      "\u001b[1B8bc8ad49: Preparing \n",
      "\u001b[1B9d1341e8: Preparing \n",
      "\u001b[1Bbf63a739: Preparing \n",
      "\u001b[1Bb1cd889d: Preparing \n",
      "\u001b[1B3cb30e82: Preparing \n",
      "\u001b[1B66ed3ac6: Preparing \n",
      "\u001b[1B62cc6fa9: Preparing \n",
      "\u001b[1Bf7474f5e: Preparing \n",
      "\u001b[1Be1a2a6ad: Preparing \n",
      "\u001b[1B4969712d: Preparing \n",
      "\u001b[1B2126cab2: Preparing \n",
      "\u001b[1B4b8590bd: Preparing \n",
      "\u001b[1Ba7dfcf02: Preparing \n",
      "\u001b[1B02007e9c: Preparing \n",
      "\u001b[1B26f01227: Preparing \n",
      "\u001b[1B0b24dbe8: Preparing \n",
      "\u001b[1B16ac41ff: Preparing \n",
      "\u001b[1B39d1c767: Preparing \n",
      "\u001b[1B46d7b29d: Preparing \n",
      "\u001b[1B5b4f5c34: Preparing \n",
      "\u001b[1Ba7535923: Preparing \n",
      "\u001b[1Bc2fc7eb9: Preparing \n",
      "\u001b[1B3ad0f1b5: Preparing \n",
      "\u001b[1B8881187d: Preparing \n",
      "\u001b[1B5df75b44: Preparing \n",
      "\u001b[1B08d5f11b: Layer already exists \u001b[21A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2Klatest: digest: sha256:505d503f181d2dd4ea89e483e4855e9440893510963c450cf8ceba4f4121a883 size: 5965\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com.cn/{}:{}'.format(aws_account_id, aws_default_region, job_name, 'latest')\n",
    "exist_image = !docker images -q {job_name}:latest 2> /dev/null\n",
    "if len(exist_image) > 0:\n",
    "    !docker tag {job_name}:latest {image_uri}\n",
    "!$(aws ecr get-login --region {aws_default_region} --no-include-email)\n",
    "print('Pushing image')\n",
    "!docker push {image_uri}\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "_i_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 | run time 9.630714 seconds \n"
     ]
    }
   ],
   "source": [
    "__start__ = timer()\n",
    "\n",
    "\n",
    "path = directory + '/'\n",
    "df_all = pd.read_csv(path + '风格因子数据.csv', converters={'stcode': str}, index_col=0)\n",
    "timelist_m = pd.read_csv(path + 'timelist_m.csv', header=None, squeeze=True)\n",
    "timelist_m = pd.to_datetime(timelist_m)\n",
    "df_all.index = pd.to_datetime(df_all.index)\n",
    "\n",
    "\n",
    "run_time = timer() - __start__\n",
    "_i_ += 1\n",
    "print(u'Part', _i_, '|', \"run time %f seconds \" % run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 | run time 0.082720 seconds \n"
     ]
    }
   ],
   "source": [
    "__start__ = timer()\n",
    "\n",
    "\n",
    "#%% 分类算法\n",
    "df_all['y_pred'] = np.nan\n",
    "df_all['y_pred_rank'] = np.nan\n",
    "df_all['group'] = np.nan\n",
    "factor_list = ['规模', '估值', '分红', '盈利', '财务质量', '成长', '反转', '波动率', '流动性', '分析师预期变化']\n",
    "importance = pd.DataFrame(index=timelist_m[: -1], columns=factor_list)\n",
    "auc = pd.DataFrame(index=timelist_m[: -1], columns=['auc'])\n",
    "window = 24\n",
    "\n",
    "\n",
    "run_time = timer() - __start__\n",
    "_i_ += 1\n",
    "print(u'Part', _i_, '|', \"run time %f seconds \" % run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始参数搜索 2010-01-29 ，共 54 组参数:PENDING JOBS: 46 RUNNING JOBS: 0 STOPPED JOBS: 0\n",
      "PENDING JOBS: 47 RUNNING JOBS: 0 STOPPED JOBS: 0\n",
      "PENDING JOBS: 49 RUNNING JOBS: 0 STOPPED JOBS: 0\n",
      "PENDING JOBS: 50 RUNNING JOBS: 0 STOPPED JOBS: 0\n",
      "PENDING JOBS: 52 RUNNING JOBS: 0 STOPPED JOBS: 1\n",
      "PENDING JOBS: 53 RUNNING JOBS: 0 STOPPED JOBS: 1\n",
      "PENDING JOBS: 53 RUNNING JOBS: 0 STOPPED JOBS: 1\n",
      "PENDING JOBS: 52 RUNNING JOBS: 1 STOPPED JOBS: 1\n",
      "PENDING JOBS: 51 RUNNING JOBS: 2 STOPPED JOBS: 1\n",
      "PENDING JOBS: 51 RUNNING JOBS: 2 STOPPED JOBS: 1\n",
      "PENDING JOBS: 49 RUNNING JOBS: 4 STOPPED JOBS: 1\n",
      "PENDING JOBS: 48 RUNNING JOBS: 5 STOPPED JOBS: 1\n",
      "PENDING JOBS: 47 RUNNING JOBS: 6 STOPPED JOBS: 1\n",
      "PENDING JOBS: 44 RUNNING JOBS: 9 STOPPED JOBS: 1\n",
      "PENDING JOBS: 43 RUNNING JOBS: 9 STOPPED JOBS: 1\n",
      "PENDING JOBS: 40 RUNNING JOBS: 11 STOPPED JOBS: 1\n",
      "PENDING JOBS: 37 RUNNING JOBS: 14 STOPPED JOBS: 1\n",
      "PENDING JOBS: 36 RUNNING JOBS: 14 STOPPED JOBS: 1\n",
      "PENDING JOBS: 35 RUNNING JOBS: 13 STOPPED JOBS: 1\n",
      "PENDING JOBS: 32 RUNNING JOBS: 16 STOPPED JOBS: 2\n",
      "PENDING JOBS: 31 RUNNING JOBS: 16 STOPPED JOBS: 3\n",
      "PENDING JOBS: 31 RUNNING JOBS: 13 STOPPED JOBS: 3\n",
      "PENDING JOBS: 31 RUNNING JOBS: 12 STOPPED JOBS: 3\n",
      "PENDING JOBS: 28 RUNNING JOBS: 15 STOPPED JOBS: 5\n",
      "PENDING JOBS: 27 RUNNING JOBS: 12 STOPPED JOBS: 6\n",
      "PENDING JOBS: 25 RUNNING JOBS: 13 STOPPED JOBS: 7\n",
      "PENDING JOBS: 24 RUNNING JOBS: 13 STOPPED JOBS: 9\n",
      "PENDING JOBS: 22 RUNNING JOBS: 15 STOPPED JOBS: 11\n",
      "PENDING JOBS: 21 RUNNING JOBS: 14 STOPPED JOBS: 11\n",
      "PENDING JOBS: 21 RUNNING JOBS: 13 STOPPED JOBS: 11\n",
      "PENDING JOBS: 19 RUNNING JOBS: 15 STOPPED JOBS: 15\n",
      "PENDING JOBS: 15 RUNNING JOBS: 17 STOPPED JOBS: 17\n",
      "PENDING JOBS: 14 RUNNING JOBS: 18 STOPPED JOBS: 17\n",
      "PENDING JOBS: 12 RUNNING JOBS: 19 STOPPED JOBS: 18\n",
      "PENDING JOBS: 11 RUNNING JOBS: 20 STOPPED JOBS: 19\n",
      "PENDING JOBS: 10 RUNNING JOBS: 20 STOPPED JOBS: 20\n",
      "PENDING JOBS: 6 RUNNING JOBS: 24 STOPPED JOBS: 21\n",
      "PENDING JOBS: 5 RUNNING JOBS: 24 STOPPED JOBS: 22\n",
      "PENDING JOBS: 4 RUNNING JOBS: 25 STOPPED JOBS: 22\n",
      "PENDING JOBS: 3 RUNNING JOBS: 23 STOPPED JOBS: 23\n",
      "PENDING JOBS: 2 RUNNING JOBS: 24 STOPPED JOBS: 23\n",
      "PENDING JOBS: 1 RUNNING JOBS: 24 STOPPED JOBS: 24\n",
      "PENDING JOBS: 0 RUNNING JOBS: 25 STOPPED JOBS: 25\n",
      "PENDING JOBS: 0 RUNNING JOBS: 24 STOPPED JOBS: 25\n",
      "PENDING JOBS: 0 RUNNING JOBS: 23 STOPPED JOBS: 25\n",
      "PENDING JOBS: 0 RUNNING JOBS: 22 STOPPED JOBS: 28\n",
      "PENDING JOBS: 0 RUNNING JOBS: 22 STOPPED JOBS: 28\n",
      "PENDING JOBS: 0 RUNNING JOBS: 21 STOPPED JOBS: 29\n",
      "PENDING JOBS: 0 RUNNING JOBS: 20 STOPPED JOBS: 29\n",
      "PENDING JOBS: 0 RUNNING JOBS: 18 STOPPED JOBS: 31\n",
      "PENDING JOBS: 0 RUNNING JOBS: 16 STOPPED JOBS: 32\n",
      "PENDING JOBS: 0 RUNNING JOBS: 15 STOPPED JOBS: 32\n",
      "PENDING JOBS: 0 RUNNING JOBS: 13 STOPPED JOBS: 32\n",
      "PENDING JOBS: 0 RUNNING JOBS: 12 STOPPED JOBS: 33\n",
      "PENDING JOBS: 0 RUNNING JOBS: 12 STOPPED JOBS: 35\n",
      "PENDING JOBS: 0 RUNNING JOBS: 12 STOPPED JOBS: 36\n",
      "PENDING JOBS: 0 RUNNING JOBS: 12 STOPPED JOBS: 38\n",
      "PENDING JOBS: 0 RUNNING JOBS: 12 STOPPED JOBS: 40\n",
      "PENDING JOBS: 0 RUNNING JOBS: 10 STOPPED JOBS: 42\n",
      "PENDING JOBS: 0 RUNNING JOBS: 10 STOPPED JOBS: 42\n",
      "PENDING JOBS: 0 RUNNING JOBS: 10 STOPPED JOBS: 42\n",
      "PENDING JOBS: 0 RUNNING JOBS: 9 STOPPED JOBS: 42\n",
      "PENDING JOBS: 0 RUNNING JOBS: 9 STOPPED JOBS: 42\n",
      "PENDING JOBS: 0 RUNNING JOBS: 8 STOPPED JOBS: 42\n",
      "PENDING JOBS: 0 RUNNING JOBS: 8 STOPPED JOBS: 44\n",
      "PENDING JOBS: 0 RUNNING JOBS: 7 STOPPED JOBS: 44\n",
      "PENDING JOBS: 0 RUNNING JOBS: 7 STOPPED JOBS: 44\n",
      "PENDING JOBS: 0 RUNNING JOBS: 7 STOPPED JOBS: 45\n",
      "PENDING JOBS: 0 RUNNING JOBS: 6 STOPPED JOBS: 46\n",
      "PENDING JOBS: 0 RUNNING JOBS: 4 STOPPED JOBS: 46\n",
      "PENDING JOBS: 0 RUNNING JOBS: 3 STOPPED JOBS: 46\n",
      "PENDING JOBS: 0 RUNNING JOBS: 3 STOPPED JOBS: 47\n",
      "PENDING JOBS: 0 RUNNING JOBS: 1 STOPPED JOBS: 47\n",
      "PENDING JOBS: 0 RUNNING JOBS: 1 STOPPED JOBS: 48\n",
      "PENDING JOBS: 0 RUNNING JOBS: 1 STOPPED JOBS: 49\n",
      "PENDING JOBS: 0 RUNNING JOBS: 1 STOPPED JOBS: 51\n",
      "PENDING JOBS: 0 RUNNING JOBS: 0 STOPPED JOBS: 51\n",
      "PENDING JOBS: 0 RUNNING JOBS: 0 STOPPED JOBS: 52\n",
      "PENDING JOBS: 0 RUNNING JOBS: 0 STOPPED JOBS: 53\n",
      "PENDING JOBS: 0 RUNNING JOBS: 0 STOPPED JOBS: 53\n",
      "PENDING JOBS: 0 RUNNING JOBS: 0 STOPPED JOBS: 53\n",
      "PENDING JOBS: 0 RUNNING JOBS: 0 STOPPED JOBS: 53\n",
      "PENDING JOBS: 0 RUNNING JOBS: 0 STOPPED JOBS: 54\n",
      "Best param: {'n_estimators': 50, 'max_depth': 4, 'min_child_weight': 1, 'gamma': 1} Score: 0.5713225394001743\n",
      "[11:11:39] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "开始参数搜索 2010-02-26 ，共 54 组参数:"
     ]
    }
   ],
   "source": [
    "__start__ = timer()\n",
    "\n",
    "\n",
    "for i in range(len(timelist_m[: -1]) - window):\n",
    "    #i=0\n",
    "    timestamp = datetime.datetime.strftime(timelist_m[i], '%Y-%m-%d')\n",
    "    df_used = df_all.loc[timelist_m[i : i + window]]\n",
    "    df_temp = pd.DataFrame(df_used[df_used['flag'] != 0])\n",
    "    df_temp['flag'] = ((df_temp['flag']+ 1)/2).astype(int).values\n",
    "    train_X = df_temp[factor_list]\n",
    "    mean = train_X.mean()\n",
    "    std = train_X.std()\n",
    "    train_X = (train_X - mean) / std # 标准化\n",
    "    train_Y = df_temp['flag']\n",
    "    \n",
    "    # 调参\n",
    "    model = XGBClassifier(random_state=0, use_label_encoder=False)\n",
    "    params = {'n_estimators': [50, 250, 500], 'max_depth': [4, 7, 10], 'min_child_weight': [0.2, 0.6, 1], 'gamma': [0.2, 1]}\n",
    "    \n",
    "    # 两两组合构成参数集\n",
    "    param_keys = list(params)\n",
    "    param_values = list(params.values())\n",
    "    param_values[0] = [[x] for x in param_values[0]]\n",
    "    combined_values = reduce(lambda x, y: [i + [j] for i in x for j in y], param_values)\n",
    "    param_list = [dict(zip(param_keys, x)) for x in combined_values]\n",
    "    task_list = []\n",
    "    \n",
    "    # 遍历参数集寻找最优参数\n",
    "    print('开始参数搜索', timestamp, '，共', len(param_list), '组参数:', end = '')\n",
    "    for i, param in enumerate(param_list):\n",
    "        s3.put_object(Body=json.dumps(param), Bucket=bucket, Key='{}/{}/trial_{}/input/param.json'.format(job_name, timestamp, str(i)))\n",
    "        s3.put_object(Body=train_X.to_csv(), Bucket=bucket, Key='{}/{}/trial_{}/input/train_X.csv'.format(job_name, timestamp, str(i)))\n",
    "        s3.put_object(Body=train_Y.to_csv(), Bucket=bucket, Key='{}/{}/trial_{}/input/train_Y.csv'.format(job_name, timestamp, str(i)))\n",
    "        r = ecs.run_task(\n",
    "            cluster='test',\n",
    "            enableExecuteCommand=False,\n",
    "            group='family:grid_search',\n",
    "            launchType='FARGATE',\n",
    "            networkConfiguration={\n",
    "                'awsvpcConfiguration': {\n",
    "                    'subnets': [\n",
    "                        'subnet-1d427657',\n",
    "                        'subnet-bc2e99d5',\n",
    "                        'subnet-1dd81066',\n",
    "                    ],\n",
    "                    'securityGroups': [\n",
    "                        'sg-0546da2f4b2ead20f',\n",
    "                    ],\n",
    "                    'assignPublicIp': 'ENABLED'\n",
    "                }\n",
    "            },\n",
    "            overrides={\n",
    "                'containerOverrides': [\n",
    "                    {\n",
    "                        'name': 'job',\n",
    "                        'environment': [\n",
    "                            {\n",
    "                                'name': 'TIMESTAMP',\n",
    "                                'value': timestamp\n",
    "                            },\n",
    "                            {\n",
    "                                'name': 'TRIAL',\n",
    "                                'value': str(i)\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "            platformVersion='1.4.0',\n",
    "            taskDefinition='arn:aws-cn:ecs:cn-northwest-1:384680359421:task-definition/grid_search:4'\n",
    "        )\n",
    "        task_list.append(r['tasks'][0]['taskArn'])\n",
    "    \n",
    "    # track 任务的执行状态\n",
    "    r = ecs.describe_tasks(cluster='test', tasks=task_list)\n",
    "    stopped_count = 0\n",
    "    while stopped_count < len(param_list):\n",
    "        pending_count = 0\n",
    "        running_count = 0\n",
    "        stopped_count = 0\n",
    "        for i in range(len(r['tasks'])):\n",
    "            if r['tasks'][i]['lastStatus'] == 'PENDING':\n",
    "                pending_count += 1\n",
    "            elif r['tasks'][i]['lastStatus'] == 'RUNNING':\n",
    "                running_count += 1\n",
    "            elif r['tasks'][i]['lastStatus'] == 'STOPPED':\n",
    "                stopped_count += 1\n",
    "        print('PENDING JOBS:', pending_count, 'RUNNING JOBS:', running_count, 'STOPPED JOBS:', stopped_count)\n",
    "        time.sleep(10)\n",
    "        r = ecs.describe_tasks(cluster='test', tasks=task_list)\n",
    "        \n",
    "    # 获取 score\n",
    "    score = []\n",
    "    for i, param in enumerate(param_list):\n",
    "        key = '{}/{}/trial_{}/output/output.json'.format(job_name, timestamp, str(i))\n",
    "        score.append(json.loads(s3.get_object(Bucket=bucket, Key=key)['Body'].read())['score'])\n",
    "    \n",
    "    # 训练模型\n",
    "    best_score = np.max(score)\n",
    "    best_param = param_list[np.argmax(score)]\n",
    "    print('Best param:', best_param, 'Score:', best_score)\n",
    "    model.set_params(**best_param)\n",
    "    model_cv = model.fit(train_X, train_Y)\n",
    "    \n",
    "    # 输出因子数据\n",
    "    temp_X_test = df_all[factor_list].loc[timelist_m[i + window]]\n",
    "    temp_X_test = (temp_X_test - mean) / std # 标准化\n",
    "    \n",
    "    auc.loc[timelist_m[i + window - 1], :] = best_score\n",
    "    importance.loc[timelist_m[i + window - 1], :] = model_cv.feature_importances_\n",
    "    y_pred_temp = pd.Series(model_cv.predict_proba(temp_X_test)[: , 1], index=temp_X_test.index)\n",
    "    df_all.loc[timelist_m[i + window], 'y_pred'] = y_pred_temp\n",
    "    df_all.loc[timelist_m[i + window], 'y_pred_rank'] = y_pred_temp.rank()\n",
    "    df_all.loc[timelist_m[i + window], 'group'] = pd.qcut(df_all.loc[timelist_m[i + window], 'y_pred'], np.arange(0, 1.1, 0.1), labels=np.arange(1, 11)).astype(int)\n",
    "\n",
    "\n",
    "run_time = timer() - __start__\n",
    "_i_ += 1\n",
    "print(u'Part', _i_, '|', \"run time %f seconds \" % run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__start__ = timer()\n",
    "\n",
    "\n",
    "df_m = df_all.loc[timelist_m[: -1]].dropna(subset=['group'])\n",
    "df_m.reset_index()[['trade_dt', 'stcode', 'y_pred']].to_csv(directory + '/机器学习合成因子_Xgboost.csv')\n",
    "\n",
    "\n",
    "run_time = timer() - __start__\n",
    "_i_ += 1\n",
    "print(u'Part', _i_, '|', \"run time %f seconds \" % run_time)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
